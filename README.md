Simple Multi-Agent Chat System

Knowledge Representation and Reasoning

Overview

This project implements a minimal multi-agent chat system designed to demonstrate fundamental concepts of Knowledge Representation and Reasoning (KR&R). The system is built around a Coordinator (Manager) agent that orchestrates multiple specialized worker agents to collaboratively answer user queries.

The implementation highlights task decomposition, inter-agent coordination, structured memory management, context-aware retrieval, and adaptive decision-making. The system operates entirely in Python and is designed to be lightweight, modular, and easily extensible.

System Architecture

The system follows a Manager–Worker multi-agent architecture, where a central Coordinator controls the execution flow and delegates tasks to specialized agents.

Coordinator (Manager)

The Coordinator acts as the central controller and is responsible for:

Receiving and interpreting user queries

Determining query complexity (simple vs. complex)

Planning and sequencing agent execution

Coordinating dependencies between agents

Querying memory to avoid redundant computation

Synthesizing final responses

Maintaining global conversation context and system state

ResearchAgent

The ResearchAgent simulates information retrieval using a pre-loaded mock knowledge base. It produces structured factual outputs enriched with metadata, including:

Topic

Source

Timestamp

Confidence score

This agent represents the information acquisition component of the system.

AnalysisAgent

The AnalysisAgent performs reasoning and analytical operations on data provided by the ResearchAgent. Its responsibilities include:

Summarization of retrieved information

Comparison of concepts and approaches

Identification of trade-offs and patterns

Production of structured analytical results with confidence scoring

MemoryAgent

The MemoryAgent manages long-term structured memory and supports context-aware retrieval. It:

Stores knowledge artifacts with rich metadata

Provides keyword-based search

Performs vector similarity–based retrieval

Enables reuse of previously learned information

Influences future decision-making by reducing redundant work

Memory Design and Retrieval

The system includes an enhanced memory layer to support persistent and context-aware knowledge management.

Memory Types
Conversation Memory

Maintains a chronological history of user queries

Enables context tracking across multiple interactions

Knowledge Base Memory

Stores structured records generated by Research and Analysis agents

Each record contains:

Topic

Content

Source agent

Timestamp

Confidence score

Agent State Memory

Tracks contributions made by each agent during task execution

Assists the Coordinator in deciding whether to reuse or recompute results

Vector-Based Retrieval

To avoid redundant computation, the MemoryAgent supports vector similarity search through a lightweight in-memory implementation:

Text is converted into deterministic numeric embeddings

Queries are embedded using the same method

Records are retrieved using cosine similarity combined with keyword matching

This approach provides functionality equivalent to FAISS or Chroma while maintaining platform compatibility and minimal external dependencies.

How to Run the System
Prerequisites

Python 3.10 or higher

Git

VS Code or any Python-supported IDE

Setup

Clone the repository:

git clone https://github.com/noorali104/krr-multiagent.git
cd krr-multiagent


Install dependencies (if required):

pip install -r requirements.txt


Note: The system primarily relies on standard Python libraries.

Run the system:

python src/main.py

Test Scenarios

The following sample scenarios are implemented and demonstrated in the outputs/ directory:

Simple Query — simple_query.txt

Complex Query — complex_query.txt

Memory Recall — memory_test.txt

Multi-step Reasoning — multi_step.txt

Collaborative Comparison — collaborative.txt

Each output file captures:

Agent collaboration

Coordinator decision-making

Memory storage and retrieval behavior

Containerization (Docker)

Docker support can be added using a standard Dockerfile and docker-compose.yml. The system is designed to be container-ready with minimal modifications, enabling consistent execution across different environments.

Notes

Integration of Large Language Models (LLMs) is optional and not required for core functionality.

Rule-based planning and reasoning serve as a fallback strategy when no external models are used.

The architecture is modular and can be easily extended with additional agents, memory strategies, or reasoning modules.
